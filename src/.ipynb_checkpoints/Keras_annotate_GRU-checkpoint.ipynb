{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, GRU, LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "import itertools\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import progressbar\n",
    "from metrics.accuracy import conlleval\n",
    "\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(rootdir):\n",
    "    text_l = []\n",
    "    label_l = []\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for fname in files:\n",
    "            #print os.path.join(subdir, file)\n",
    "            filepath = subdir + os.sep + fname\n",
    "            with open(filepath) as f:\n",
    "                for line1,line2 in itertools.izip_longest(*[f]*2):\n",
    "                    try:\n",
    "                        text = ast.literal_eval(line1)\n",
    "                        label = ast.literal_eval(line2)\n",
    "                        if len(text) > 2:\n",
    "                            text_l.append(text)\n",
    "                            label_l.append(label)\n",
    "                    except:\n",
    "                        pass\n",
    "            f.close()\n",
    "    return text_l, label_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5426 5426\n",
      "{0: 'GO:0000267', 1: 'GO:0000502', 2: 'GO:0000775', 3: 'GO:0000785', 4: 'GO:0000786', 5: 'GO:0000791', 6: 'GO:0000792', 7: 'GO:0000795', 8: 'GO:0000805', 9: 'GO:0000806', 10: 'GO:0000811', 11: 'GO:0001669', 12: 'GO:0001750', 13: 'GO:0001917', 14: 'GO:0005575', 15: 'GO:0005576', 16: 'GO:0005577', 17: 'GO:0005581', 18: 'GO:0005585', 19: 'GO:0005610', 20: 'GO:0005622', 21: 'GO:0005623', 22: 'GO:0005634', 23: 'GO:0005643', 24: 'GO:0005654', 25: 'GO:0005656', 26: 'GO:0005657', 27: 'GO:0005675', 28: 'GO:0005694', 29: 'GO:0005712', 30: 'GO:0005730', 31: 'GO:0005737', 32: 'GO:0005739', 33: 'GO:0005764', 34: 'GO:0005768', 35: 'GO:0005773', 36: 'GO:0005776', 37: 'GO:0005777', 38: 'GO:0005783', 39: 'GO:0005792', 40: 'GO:0005813', 41: 'GO:0005819', 42: 'GO:0005829', 43: 'GO:0005833', 44: 'GO:0005835', 45: 'GO:0005840', 46: 'GO:0005856', 47: 'GO:0005871', 48: 'GO:0005874', 49: 'GO:0005883', 50: 'GO:0005886', 51: 'GO:0005901', 52: 'GO:0005902', 53: 'GO:0005912', 54: 'GO:0005929', 55: 'GO:0005966', 56: 'GO:0008091', 57: 'GO:0008305', 58: 'GO:0009986', 59: 'GO:0010369', 60: 'GO:0014069', 61: 'GO:0016020', 62: 'GO:0016021', 63: 'GO:0016028', 64: 'GO:0016234', 65: 'GO:0016459', 66: 'GO:0016528', 67: 'GO:0017086', 68: 'GO:0019814', 69: 'GO:0030016', 70: 'GO:0030054', 71: 'GO:0030056', 72: 'GO:0030286', 73: 'GO:0030424', 74: 'GO:0030425', 75: 'GO:0030849', 76: 'GO:0031012', 77: 'GO:0031143', 78: 'GO:0031240', 79: 'GO:0031965', 80: 'GO:0031982', 81: 'GO:0032391', 82: 'GO:0032991', 83: 'GO:0035102', 84: 'GO:0042470', 85: 'GO:0042555', 86: 'GO:0042611', 87: 'GO:0042995', 88: 'GO:0043025', 89: 'GO:0043204', 90: 'GO:0043209', 91: 'GO:0043226', 92: 'GO:0043256', 93: 'GO:0043626', 94: 'GO:0043679', 95: 'GO:0045120', 96: 'GO:0045177', 97: 'GO:0045202', 98: 'GO:0045251', 99: 'GO:0045277', 100: 'GO:0045298', 101: 'GO:0046581', 102: 'GO:0048471', 103: 'NA'}\n"
     ]
    }
   ],
   "source": [
    "X_text, y_text = read_data('../data/')\n",
    "\n",
    "print len(X_text), len(y_text)\n",
    "\n",
    "text = list(set(itertools.chain(*X_text)))\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(text)\n",
    "\n",
    "w2idx = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "idx2w  = {w2idx[k]:k for k in w2idx}\n",
    "\n",
    "X = []\n",
    "for each in X_text:\n",
    "    X.append(le.transform(each))\n",
    "\n",
    "label = list(set(itertools.chain(*y_text)))\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(label)\n",
    "\n",
    "labels2idx = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "idx2la = {labels2idx[k]:k for k in labels2idx}\n",
    "\n",
    "y = []\n",
    "for each in y_text:\n",
    "    y.append(le.transform(each))\n",
    "\n",
    "print idx2la\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "words_val = [ list(map(lambda x: idx2w[x], w)) for w in X_test]\n",
    "groundtruth_val = [ list(map(lambda x: idx2la[x], y)) for y in y_test] # y values test\n",
    "words_train = [ list(map(lambda x: idx2w[x], w)) for w in X_train]\n",
    "groundtruth_train = [ list(map(lambda x: idx2la[x], y)) for y in y_train] # y values train\n",
    "\n",
    "n_classes = len(idx2la)\n",
    "n_vocab = len(idx2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GO:0000267' 'GO:0000502' 'GO:0000775' 'GO:0000785' 'GO:0000786'\n",
      " 'GO:0000791' 'GO:0000792' 'GO:0000795' 'GO:0000805' 'GO:0000806'\n",
      " 'GO:0000811' 'GO:0001669' 'GO:0001750' 'GO:0001917' 'GO:0005575'\n",
      " 'GO:0005576' 'GO:0005577' 'GO:0005581' 'GO:0005585' 'GO:0005610'\n",
      " 'GO:0005622' 'GO:0005623' 'GO:0005634' 'GO:0005643' 'GO:0005654'\n",
      " 'GO:0005656' 'GO:0005657' 'GO:0005675' 'GO:0005694' 'GO:0005712'\n",
      " 'GO:0005730' 'GO:0005737' 'GO:0005739' 'GO:0005764' 'GO:0005768'\n",
      " 'GO:0005773' 'GO:0005776' 'GO:0005777' 'GO:0005783' 'GO:0005792'\n",
      " 'GO:0005813' 'GO:0005819' 'GO:0005829' 'GO:0005833' 'GO:0005835'\n",
      " 'GO:0005840' 'GO:0005856' 'GO:0005871' 'GO:0005874' 'GO:0005883'\n",
      " 'GO:0005886' 'GO:0005901' 'GO:0005902' 'GO:0005912' 'GO:0005929'\n",
      " 'GO:0005966' 'GO:0008091' 'GO:0008305' 'GO:0009986' 'GO:0010369'\n",
      " 'GO:0014069' 'GO:0016020' 'GO:0016021' 'GO:0016028' 'GO:0016234'\n",
      " 'GO:0016459' 'GO:0016528' 'GO:0017086' 'GO:0019814' 'GO:0030016'\n",
      " 'GO:0030054' 'GO:0030056' 'GO:0030286' 'GO:0030424' 'GO:0030425'\n",
      " 'GO:0030849' 'GO:0031012' 'GO:0031143' 'GO:0031240' 'GO:0031965'\n",
      " 'GO:0031982' 'GO:0032391' 'GO:0032991' 'GO:0035102' 'GO:0042470'\n",
      " 'GO:0042555' 'GO:0042611' 'GO:0042995' 'GO:0043025' 'GO:0043204'\n",
      " 'GO:0043209' 'GO:0043226' 'GO:0043256' 'GO:0043626' 'GO:0043679'\n",
      " 'GO:0045120' 'GO:0045177' 'GO:0045202' 'GO:0045251' 'GO:0045277'\n",
      " 'GO:0045298' 'GO:0046581' 'GO:0048471' 'NA']\n"
     ]
    }
   ],
   "source": [
    "print le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence : ['Total', 'RNA', 'was', 'extracted', 'from', 'the', 'entire', 'small', 'intestine', 'as', 'previously', 'described', '', 'Expression', 'of', 'the', 'major', 'intestinal', 'mucin', 'Muc2', 'was', 'also', 'measured', 'using', 'the', 'forward', '5aposGAC', 'TTC', 'GAT', 'GGA', 'CAC', 'TGC', 'TC3apos', 'and', 'reverse', '5aposCAC', 'GGT', 'GTT', 'TAT', 'CTA', 'CCA', 'AC3apos', 'primers']\n",
      "Encoded form: [10996  9386 23920 15902 16311 23060 15569 22251 17663 12628 20680 14746\n",
      "     0  5372 19504 23060 18344 17661 18968  8007 23920 12108 18480 23728\n",
      " 23060 16264  1597 10741  5803  5861  3656 10646 10611 12218 21605  1584\n",
      "  5871  5922 10597  3832  3691  2323 20695]\n",
      "\n",
      "It's label : ['NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA']\n",
      "Encoded form: [103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103\n",
      " 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103\n",
      " 103 103 103 103 103 103 103]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example sentence : {}\".format(words_train[0]))\n",
    "print(\"Encoded form: {}\".format(X_train[0]))\n",
    "print \n",
    "print(\"It's label : {}\".format(groundtruth_train[0]))\n",
    "print(\"Encoded form: {}\".format(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdmohant/.virtualenvs/deeplearn/lib/python2.7/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 5, padding=\"same\", activation=\"relu\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab,100))\n",
    "model.add(Convolution1D(64,5,border_mode='same', activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(GRU(100,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n",
    "model.compile('rmsprop', 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.0, nb_epoch=100,\n",
    "                        batch_size=100, verbose=1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training =>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "  0% |                                                                        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating =>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95% |####################################################################    |\r"
     ]
    }
   ],
   "source": [
    "### Training\n",
    "n_epochs = 50\n",
    "\n",
    "train_f_scores = []\n",
    "val_f_scores = []\n",
    "best_val_f1 = 0\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    print(\"Epoch {}\".format(i))\n",
    "    \n",
    "    print(\"Training =>\")\n",
    "    train_pred_label = []\n",
    "    avgLoss = 0\n",
    "\n",
    "\n",
    "    bar = progressbar.ProgressBar(maxval=len(X_train))\n",
    "    for n_batch, sent in bar(enumerate(X_train)):\n",
    "        label = y_train[n_batch]\n",
    "        label = np.eye(n_classes)[label][np.newaxis,:]\n",
    "        sent = sent[np.newaxis,:]\n",
    "\n",
    "        if sent.shape[1] > 1: #some bug in keras\n",
    "            loss = model.train_on_batch(sent, label)\n",
    "            avgLoss += loss\n",
    "\n",
    "        pred = model.predict_on_batch(sent)\n",
    "        pred = np.argmax(pred,-1)[0]\n",
    "        train_pred_label.append(pred)\n",
    "\n",
    "    avgLoss = avgLoss/n_batch\n",
    "    \n",
    "    predword_train = [ list(map(lambda x: idx2la[x], y)) for y in train_pred_label]\n",
    "    \n",
    "    \n",
    "#     con_dict = conlleval(predword_train, groundtruth_train, words_train, 'r.txt')\n",
    "#     print con_dict\n",
    "#     train_f_scores.append(con_dict['f1'])\n",
    "    \n",
    "\n",
    "#     print classification_report(list(itertools.chain.from_iterable(groundtruth_train)), \n",
    "#                                 list(itertools.chain.from_iterable(predword_train)))\n",
    "    \n",
    "#     print('Loss = {}, Precision = {}, Recall = {}, F1 = {}'.format(avgLoss, con_dict['r'], con_dict['p'], con_dict['f1']))\n",
    "    \n",
    "    \n",
    "    print(\"Validating =>\")\n",
    "    \n",
    "    val_pred_label = []\n",
    "    avgLoss = 0\n",
    "    \n",
    "    bar = progressbar.ProgressBar(maxval=len(X_test))\n",
    "    for n_batch, sent in bar(enumerate(X_test)):\n",
    "        label = y_test[n_batch]\n",
    "        label = np.eye(n_classes)[label][np.newaxis,:]\n",
    "        sent = sent[np.newaxis,:]\n",
    "        \n",
    "        if sent.shape[1] > 1: #some bug in keras\n",
    "            loss = model.test_on_batch(sent, label)\n",
    "            avgLoss += loss\n",
    "\n",
    "        pred = model.predict_on_batch(sent)\n",
    "        pred = np.argmax(pred,-1)[0]\n",
    "        val_pred_label.append(pred)\n",
    "\n",
    "    avgLoss = avgLoss/n_batch\n",
    "    \n",
    "    predword_val = [ list(map(lambda x: idx2la[x], y)) for y in val_pred_label]\n",
    "#     con_dict = conlleval(predword_val, groundtruth_val, words_val, 'r.txt')\n",
    "#     print con_dict\n",
    "#     val_f_scores.append(con_dict['f1'])\n",
    "    \n",
    "#     print('Loss = {}, Precision = {}, Recall = {}, F1 = {}'.format(avgLoss, con_dict['r'], con_dict['p'], con_dict['f1']))\n",
    "\n",
    "print classification_report(list(itertools.chain.from_iterable(groundtruth_val)), \n",
    "                            list(itertools.chain.from_iterable(predword_val)))\n",
    "\n",
    "    \n",
    "#     if con_dict['f1'] > best_val_f1:\n",
    "#     \tbest_val_f1 = con_dict['f1']\n",
    "#     \topen('model_architecture.json','w').write(model.to_json())\n",
    "#     \tmodel.save_weights('best_model_weights.h5',overwrite=True)\n",
    "#     \tprint(\"Best validation F1 score = {}\".format(best_val_f1))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
