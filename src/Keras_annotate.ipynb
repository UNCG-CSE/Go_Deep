{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, GRU, LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "import itertools\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import progressbar\n",
    "from metrics.accuracy import conlleval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_data(rootdir):\n",
    "    text_l = []\n",
    "    label_l = []\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for fname in files:\n",
    "            #print os.path.join(subdir, file)\n",
    "            filepath = subdir + os.sep + fname\n",
    "            with open(filepath) as f:\n",
    "                for line1,line2 in itertools.izip_longest(*[f]*2):\n",
    "                    try:\n",
    "                        text = ast.literal_eval(line1)\n",
    "                        label = ast.literal_eval(line2)\n",
    "                        if len(text) > 2:\n",
    "                            text_l.append(text)\n",
    "                            label_l.append(label)\n",
    "                    except:\n",
    "                        pass\n",
    "            f.close()\n",
    "    return text_l, label_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5426 5426\n",
      "{0: 'GO:0000267', 1: 'GO:0000502', 2: 'GO:0000775', 3: 'GO:0000785', 4: 'GO:0000786', 5: 'GO:0000791', 6: 'GO:0000792', 7: 'GO:0000795', 8: 'GO:0000805', 9: 'GO:0000806', 10: 'GO:0000811', 11: 'GO:0001669', 12: 'GO:0001750', 13: 'GO:0001917', 14: 'GO:0005575', 15: 'GO:0005576', 16: 'GO:0005577', 17: 'GO:0005581', 18: 'GO:0005585', 19: 'GO:0005610', 20: 'GO:0005622', 21: 'GO:0005623', 22: 'GO:0005634', 23: 'GO:0005643', 24: 'GO:0005654', 25: 'GO:0005656', 26: 'GO:0005657', 27: 'GO:0005675', 28: 'GO:0005694', 29: 'GO:0005712', 30: 'GO:0005730', 31: 'GO:0005737', 32: 'GO:0005739', 33: 'GO:0005764', 34: 'GO:0005768', 35: 'GO:0005773', 36: 'GO:0005776', 37: 'GO:0005777', 38: 'GO:0005783', 39: 'GO:0005792', 40: 'GO:0005813', 41: 'GO:0005819', 42: 'GO:0005829', 43: 'GO:0005833', 44: 'GO:0005835', 45: 'GO:0005840', 46: 'GO:0005856', 47: 'GO:0005871', 48: 'GO:0005874', 49: 'GO:0005883', 50: 'GO:0005886', 51: 'GO:0005901', 52: 'GO:0005902', 53: 'GO:0005912', 54: 'GO:0005929', 55: 'GO:0005966', 56: 'GO:0008091', 57: 'GO:0008305', 58: 'GO:0009986', 59: 'GO:0010369', 60: 'GO:0014069', 61: 'GO:0016020', 62: 'GO:0016021', 63: 'GO:0016028', 64: 'GO:0016234', 65: 'GO:0016459', 66: 'GO:0016528', 67: 'GO:0017086', 68: 'GO:0019814', 69: 'GO:0030016', 70: 'GO:0030054', 71: 'GO:0030056', 72: 'GO:0030286', 73: 'GO:0030424', 74: 'GO:0030425', 75: 'GO:0030849', 76: 'GO:0031012', 77: 'GO:0031143', 78: 'GO:0031240', 79: 'GO:0031965', 80: 'GO:0031982', 81: 'GO:0032391', 82: 'GO:0032991', 83: 'GO:0035102', 84: 'GO:0042470', 85: 'GO:0042555', 86: 'GO:0042611', 87: 'GO:0042995', 88: 'GO:0043025', 89: 'GO:0043204', 90: 'GO:0043209', 91: 'GO:0043226', 92: 'GO:0043256', 93: 'GO:0043626', 94: 'GO:0043679', 95: 'GO:0045120', 96: 'GO:0045177', 97: 'GO:0045202', 98: 'GO:0045251', 99: 'GO:0045277', 100: 'GO:0045298', 101: 'GO:0046581', 102: 'GO:0048471', 103: 'NA'}\n"
     ]
    }
   ],
   "source": [
    "X_text, y_text = read_data('../data/')\n",
    "\n",
    "print len(X_text), len(y_text)\n",
    "\n",
    "text = list(set(itertools.chain(*X_text)))\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(text)\n",
    "\n",
    "w2idx = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "idx2w  = {w2idx[k]:k for k in w2idx}\n",
    "\n",
    "X = []\n",
    "for each in X_text:\n",
    "    X.append(le.transform(each))\n",
    "\n",
    "label = list(set(itertools.chain(*y_text)))\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(label)\n",
    "\n",
    "labels2idx = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "idx2la = {labels2idx[k]:k for k in labels2idx}\n",
    "\n",
    "y = []\n",
    "for each in y_text:\n",
    "    y.append(le.transform(each))\n",
    "\n",
    "print idx2la\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "words_val = [ list(map(lambda x: idx2w[x], w)) for w in X_test]\n",
    "groundtruth_val = [ list(map(lambda x: idx2la[x], y)) for y in y_test] # y values test\n",
    "words_train = [ list(map(lambda x: idx2w[x], w)) for w in X_train]\n",
    "groundtruth_train = [ list(map(lambda x: idx2la[x], y)) for y in y_train] # y values train\n",
    "\n",
    "n_classes = len(idx2la)\n",
    "n_vocab = len(idx2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GO:0000267' 'GO:0000502' 'GO:0000775' 'GO:0000785' 'GO:0000786'\n",
      " 'GO:0000791' 'GO:0000792' 'GO:0000795' 'GO:0000805' 'GO:0000806'\n",
      " 'GO:0000811' 'GO:0001669' 'GO:0001750' 'GO:0001917' 'GO:0005575'\n",
      " 'GO:0005576' 'GO:0005577' 'GO:0005581' 'GO:0005585' 'GO:0005610'\n",
      " 'GO:0005622' 'GO:0005623' 'GO:0005634' 'GO:0005643' 'GO:0005654'\n",
      " 'GO:0005656' 'GO:0005657' 'GO:0005675' 'GO:0005694' 'GO:0005712'\n",
      " 'GO:0005730' 'GO:0005737' 'GO:0005739' 'GO:0005764' 'GO:0005768'\n",
      " 'GO:0005773' 'GO:0005776' 'GO:0005777' 'GO:0005783' 'GO:0005792'\n",
      " 'GO:0005813' 'GO:0005819' 'GO:0005829' 'GO:0005833' 'GO:0005835'\n",
      " 'GO:0005840' 'GO:0005856' 'GO:0005871' 'GO:0005874' 'GO:0005883'\n",
      " 'GO:0005886' 'GO:0005901' 'GO:0005902' 'GO:0005912' 'GO:0005929'\n",
      " 'GO:0005966' 'GO:0008091' 'GO:0008305' 'GO:0009986' 'GO:0010369'\n",
      " 'GO:0014069' 'GO:0016020' 'GO:0016021' 'GO:0016028' 'GO:0016234'\n",
      " 'GO:0016459' 'GO:0016528' 'GO:0017086' 'GO:0019814' 'GO:0030016'\n",
      " 'GO:0030054' 'GO:0030056' 'GO:0030286' 'GO:0030424' 'GO:0030425'\n",
      " 'GO:0030849' 'GO:0031012' 'GO:0031143' 'GO:0031240' 'GO:0031965'\n",
      " 'GO:0031982' 'GO:0032391' 'GO:0032991' 'GO:0035102' 'GO:0042470'\n",
      " 'GO:0042555' 'GO:0042611' 'GO:0042995' 'GO:0043025' 'GO:0043204'\n",
      " 'GO:0043209' 'GO:0043226' 'GO:0043256' 'GO:0043626' 'GO:0043679'\n",
      " 'GO:0045120' 'GO:0045177' 'GO:0045202' 'GO:0045251' 'GO:0045277'\n",
      " 'GO:0045298' 'GO:0046581' 'GO:0048471' 'NA']\n"
     ]
    }
   ],
   "source": [
    "print le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence : ['The', 'S2', 'pocket', 'in', 'CLN2', 'is', 'also', 'quite', 'open', 'and', 'accessible', 'to', 'solvent', 'It', 'is', 'most', 'likely', 'larger', 'than', 'the', 'equivalent', 'pockets', 'in', 'either', 'sedolisin', 'or', 'kumamolisin', 'since', 'these', 'are', 'limited', 'by', 'Trp81', 'in', 'the', 'former', 'and', 'Trp129', 'in', 'the', 'latter', 'these', 'residues', 'originate', 'from', 'different', 'parts', 'of', 'the', 'backbone', 'in', 'the', 'two', 'enzymes', 'and', 'are', 'not', 'topologically', 'related', 'Tyr130', 'an', 'equivalent', 'of', 'the', 'latter', 'residue', 'in', 'CLN2', 'is', 'unlikely', 'to', 'come', 'into', 'direct', 'contact', 'with', 'the', 'P2', 'residue', 'of', 'the', 'substrate', 'due', 'to', 'its', 'greater', 'distance', 'almost', '4', '\\xc3\\x85', 'for', 'the', 'closest', 'atoms']\n",
      "Encoded form: [10904  9720 20395 17318  3771 17763 12108 21032 19565 12218 11799 23196\n",
      " 22289  6850 17763 18927 18080 17944 23055 23060 15651 20396 17318 15371\n",
      " 21907 19600 17891 22164 23088 12573 18092 13219 11099 17318 23060 16254\n",
      " 12218 11097 17318 23060 17969 23088 21505 19639 16311 14860 19998 19504\n",
      " 23060 12814 17318 23060 23485 15590 12218 12573 19380 23219 21354 11135\n",
      " 12184 15651 19504 23060 17969 21504 17318  3771 17763 23641 23196 13860\n",
      " 17666 14930 14111 24034 23060  8621 21504 19504 23060 22707 15248 23196\n",
      " 17796 16614 15050 12092  1247 24158 16223 23060 13713 12698]\n",
      "\n",
      "It's label : ['NA', 'NA', 'NA', 'NA', 'NA', 'NA']\n",
      "Encoded form: [103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103\n",
      " 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103\n",
      " 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103\n",
      " 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103\n",
      " 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103 103\n",
      " 103 103 103 103]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example sentence : {}\".format(words_train[0]))\n",
    "print(\"Encoded form: {}\".format(X_train[0]))\n",
    "print \n",
    "print(\"It's label : {}\".format(groundtruth_val[0]))\n",
    "print(\"Encoded form: {}\".format(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/somya/Virtualenvs/ml/lib/python2.7/site-packages/ipykernel/__main__.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 5, padding=\"same\", activation=\"relu\")`\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab,100))\n",
    "model.add(Convolution1D(64,5,border_mode='same', activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(GRU(100,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n",
    "model.compile('rmsprop', 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      "N/A% (0 of 3798) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training =>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9% (353 of 3798) |##                    | Elapsed Time: 0:00:53 ETA:  0:10:11"
     ]
    }
   ],
   "source": [
    "### Training\n",
    "n_epochs = 100\n",
    "\n",
    "train_f_scores = []\n",
    "val_f_scores = []\n",
    "best_val_f1 = 0\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    print(\"Epoch {}\".format(i))\n",
    "    \n",
    "    print(\"Training =>\")\n",
    "    train_pred_label = []\n",
    "    avgLoss = 0\n",
    "\n",
    "\n",
    "    bar = progressbar.ProgressBar(maxval=len(X_train))\n",
    "    for n_batch, sent in bar(enumerate(X_train)):\n",
    "        label = y_train[n_batch]\n",
    "        label = np.eye(n_classes)[label][np.newaxis,:]\n",
    "        sent = sent[np.newaxis,:]\n",
    "\n",
    "        if sent.shape[1] > 1: #some bug in keras\n",
    "            loss = model.train_on_batch(sent, label)\n",
    "            avgLoss += loss\n",
    "\n",
    "        pred = model.predict_on_batch(sent)\n",
    "        pred = np.argmax(pred,-1)[0]\n",
    "        train_pred_label.append(pred)\n",
    "\n",
    "    avgLoss = avgLoss/n_batch\n",
    "    \n",
    "    predword_train = [ list(map(lambda x: idx2la[x], y)) for y in train_pred_label]\n",
    "    \n",
    "    \n",
    "#     con_dict = conlleval(predword_train, groundtruth_train, words_train, 'r.txt')\n",
    "#     print con_dict\n",
    "#     train_f_scores.append(con_dict['f1'])\n",
    "    \n",
    "\n",
    "    print classification_report(list(itertools.chain.from_iterable(groundtruth_train)), \n",
    "                                list(itertools.chain.from_iterable(predword_train)))\n",
    "    \n",
    "#     print('Loss = {}, Precision = {}, Recall = {}, F1 = {}'.format(avgLoss, con_dict['r'], con_dict['p'], con_dict['f1']))\n",
    "    \n",
    "    \n",
    "    print(\"Validating =>\")\n",
    "    \n",
    "    val_pred_label = []\n",
    "    avgLoss = 0\n",
    "    \n",
    "    bar = progressbar.ProgressBar(maxval=len(X_test))\n",
    "    for n_batch, sent in bar(enumerate(X_test)):\n",
    "        label = y_test[n_batch]\n",
    "        label = np.eye(n_classes)[label][np.newaxis,:]\n",
    "        sent = sent[np.newaxis,:]\n",
    "        \n",
    "        if sent.shape[1] > 1: #some bug in keras\n",
    "            loss = model.test_on_batch(sent, label)\n",
    "            avgLoss += loss\n",
    "\n",
    "        pred = model.predict_on_batch(sent)\n",
    "        pred = np.argmax(pred,-1)[0]\n",
    "        val_pred_label.append(pred)\n",
    "\n",
    "    avgLoss = avgLoss/n_batch\n",
    "    \n",
    "    predword_val = [ list(map(lambda x: idx2la[x], y)) for y in val_pred_label]\n",
    "#     con_dict = conlleval(predword_val, groundtruth_val, words_val, 'r.txt')\n",
    "#     print con_dict\n",
    "#     val_f_scores.append(con_dict['f1'])\n",
    "    \n",
    "#     print('Loss = {}, Precision = {}, Recall = {}, F1 = {}'.format(avgLoss, con_dict['r'], con_dict['p'], con_dict['f1']))\n",
    "\n",
    "    print classification_report(list(itertools.chain.from_iterable(groundtruth_val)), \n",
    "                                list(itertools.chain.from_iterable(predword_val)))\n",
    "\n",
    "    \n",
    "#     if con_dict['f1'] > best_val_f1:\n",
    "#     \tbest_val_f1 = con_dict['f1']\n",
    "#     \topen('model_architecture.json','w').write(model.to_json())\n",
    "#     \tmodel.save_weights('best_model_weights.h5',overwrite=True)\n",
    "#     \tprint(\"Best validation F1 score = {}\".format(best_val_f1))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
