{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, GRU, LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import itertools\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys, os\n",
    "import traceback\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import progressbar\n",
    "\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(rootdir):\n",
    "    data_list = []\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for fname in files:\n",
    "            #print os.path.join(subdir, file)\n",
    "            filepath = subdir + os.sep + fname\n",
    "            with open(filepath) as f:\n",
    "                for line1,line2 in itertools.izip_longest(*[f]*2):\n",
    "                    try:\n",
    "                        text = ast.literal_eval(line1)\n",
    "                        label = ast.literal_eval(line2)\n",
    "                        \n",
    "#                         for i in range(len(label)):\n",
    "#                             if label[i] == 'NA':\n",
    "#                                 label[i] = 'O'\n",
    "                        n_labels = set(label)\n",
    "                        if len(text) > 3 and len(n_labels) > 1:\n",
    "                            sentence = zip(text, label)\n",
    "                            sentence.append(('.', 'O'))\n",
    "                            sentence = [item for item in sentence if item[0].strip() != '']\n",
    "                            data_list.append(sentence)\n",
    "                    except:\n",
    "                        pass\n",
    "            f.close()\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'O'), ('Hybrid', 'O'), ('Photoreceptor', 'O'), ('Expressing', 'O'), ('Both', 'O'), ('Rod', 'O'), ('and', 'O'), ('Cone', 'O'), ('Genes', 'SO:0000704'), ('in', 'O'), ('a', 'O'), ('Mouse', 'O'), ('Model', 'O'), ('of', 'O'), ('Enhanced', 'O'), ('SCone', 'O'), ('Syndrome', 'O'), ('.', 'O')]\n",
      "7262\n",
      "15606\n",
      "156\n"
     ]
    }
   ],
   "source": [
    "data_list = read_data('../data/input/output_so/')\n",
    "\n",
    "print data_list[0]\n",
    "\n",
    "# data_list = data_list[:3000]\n",
    "\n",
    "# words = list(chain.from_iterable(data_list))\n",
    "# print words\n",
    "\n",
    "print len(data_list)\n",
    "\n",
    "words = list(set(y[0] for x in data_list for y in x))\n",
    "words.append(\"ENDPAD\")\n",
    "n_words = len(words)\n",
    "print n_words\n",
    "\n",
    "tags = list(set(y[1] for x in data_list for y in x))\n",
    "n_tags = len(tags)\n",
    "print n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# le.fit(list(y[1] for x in data_list for y in x))\n",
    "\n",
    "# labels2idx = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "# idx2la = {labels2idx[k]:k for k in labels2idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 75\n",
    "max_len_char = 10\n",
    "\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1\n",
    "word2idx[\"O\"] = 0\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "tag2idx = {t: i + 1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"O\"] = 0\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "\n",
    "# print word2idx['Hybrid']\n",
    "# print tag2idx[\"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SO:0000994': 1, 'SO:0000993': 2, 'SO:0000188': 3, 'SO:0000159': 87, 'SO:0000855': 4, 'SO:0000854': 5, 'SO:0000857': 6, 'SO:0000856': 7, 'SO:0000851': 8, 'SO:0000853': 9, 'SO:0000667': 10, 'SO:0001077': 11, 'SO:0000859': 12, 'SO:0000858': 13, 'SO:0001452': 14, 'SO:0001453': 15, 'SO:0001450': 16, 'SO:0001451': 17, 'SO:0001454': 18, 'SO:1000035': 19, 'SO:0000051': 20, 'SO:0000345': 21, 'SO:0000346': 22, 'SO:0000110': 23, 'SO:0000340': 24, 'SO:0000112': 25, 'SO:0000296': 26, 'SO:0001183': 27, 'SO:0001185': 28, 'SO:0001059': 29, 'SO:0000860': 30, 'SO:0000865': 31, 'SO:0000902': 32, 'SO:0001442': 33, 'SO:0000906': 34, 'SO:0000673': 35, 'SO:0001444': 36, 'SO:0001441': 37, 'SO:0001114': 38, 'SO:0001445': 39, 'SO:0000771': 40, 'SO:0001447': 41, 'SO:0001446': 42, 'SO:0000068': 43, 'SO:0001448': 44, 'SO:0000104': 45, 'SO:0000984': 46, 'SO:0000577': 47, 'SO:0000879': 48, 'SO:0000646': 50, 'SO:0001025': 153, 'SO:0000077': 53, 'SO:0000360': 54, 'SO:0000407': 55, 'SO:0000988': 56, 'SO:0000985': 57, 'SO:0000409': 58, 'O': 0, 'SO:0000657': 60, 'SO:0000717': 61, 'SO:0000806': 62, 'SO:0000156': 63, 'SO:0000165': 64, 'SO:0000330': 106, 'SO:0000167': 66, 'SO:0001062': 67, 'SO:0001060': 68, 'SO:0000417': 69, 'SO:0000319': 70, 'SO:0000006': 71, 'SO:0000005': 72, 'SO:0000313': 73, 'SO:0000316': 74, 'SO:0000551': 75, 'SO:0001236': 151, 'SO:0000704': 76, 'SO:0000814': 78, 'SO:0000817': 79, 'SO:0000151': 80, 'SO:0000153': 81, 'SO:0000154': 82, 'SO:0000155': 83, 'SO:0000624': 84, 'SO:0000625': 85, 'SO:0000253': 86, 'SO:0000252': 49, 'SO:0001237': 152, 'SO:0001018': 88, 'SO:0000546': 89, 'SO:0000782': 90, 'SO:1000002': 91, 'SO:0000789': 92, 'SO:0000343': 93, 'SO:0000730': 94, 'SO:0000731': 95, 'SO:0000732': 96, 'SO:0001407': 98, 'SO:0000449': 127, 'SO:0000147': 99, 'SO:0005853': 112, 'SO:0000207': 142, 'SO:0000149': 103, 'SO:0000830': 114, 'SO:0000336': 105, 'SO:0000028': 65, 'SO:0000331': 107, 'independent_continuant': 108, 'SO:0100014': 109, 'SO:0100015': 110, 'SO:0005852': 111, 'SO:0000726': 100, 'SO:0000725': 113, 'SO:0005851': 104, 'SO:0001435': 115, 'SO:0001436': 116, 'SO:0001437': 117, 'SO:0001438': 118, 'SO:0001439': 119, 'SO:0000933': 120, 'SO:0001023': 148, 'SO:0000357': 149, 'SO:0000234': 123, 'SO:0000236': 124, 'SO:0001230': 150, 'SO:0001248': 126, 'SO:0000001': 51, 'SO:0001027': 128, 'SO:0000440': 129, 'SO:0001037': 130, 'SO:0001030': 131, 'SO:0001031': 132, 'SO:0000696': 133, 'SO:0000199': 52, 'SO:0000351': 154, 'SO:0000243': 135, 'SO:0000694': 136, 'SO:0000610': 137, 'SO:0000203': 138, 'SO:0000756': 139, 'SO:0000205': 140, 'SO:0000048': 141, 'SO:0000049': 101, 'SO:0000046': 143, 'SO:0000047': 144, 'SO:0000045': 145, 'SO:0000043': 146, 'SO:0000041': 147, 'SO:0000356': 121, 'SO:0000289': 122, 'SO:0001021': 125, 'SO:0000352': 102, 'SO:0001026': 77, 'SO:0000350': 134, 'SO:0001024': 97, 'SO:0000699': 155, 'SO:0000359': 156}\n"
     ]
    }
   ],
   "source": [
    "print tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_word = [[word2idx[w[0]] for w in s] for s in data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_word = pad_sequences(maxlen=max_len, sequences=X_word, value=word2idx[\"O\"], padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "chars = set([w_i for w in words for w_i in w])\n",
    "n_chars = len(chars)\n",
    "print(n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char2idx = {c: i + 2 for i, c in enumerate(chars)}\n",
    "char2idx[\"UNK\"] = 1\n",
    "char2idx[\"O\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_char = []\n",
    "for sentence in data_list:\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                word_seq.append(char2idx.get(sentence[i][0][j]))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"PAD\"))\n",
    "        sent_seq.append(word_seq)\n",
    "    X_char.append(np.array(sent_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = [[tag2idx[w[1]] for w in s] for s in data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = pad_sequences(maxlen=max_len, sequences=y,value=tag2idx[\"O\"], padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_word_tr, X_word_te, y_tr, y_te = train_test_split(X_word, y, test_size=0.3, random_state=2018)\n",
    "X_char_tr, X_char_te, _, _ = train_test_split(X_char, y, test_size=0.3, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D\n",
    "from keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
    "\n",
    "# input and embedding for words\n",
    "word_in = Input(shape=(max_len,))\n",
    "emb_word = Embedding(input_dim=n_words + 2, output_dim=100,\n",
    "                     input_length=max_len, mask_zero=True)(word_in)\n",
    "\n",
    "# input and embeddings for characters\n",
    "char_in = Input(shape=(max_len, max_len_char,))\n",
    "emb_char = TimeDistributed(Embedding(input_dim=n_chars + 2, output_dim=100,\n",
    "                           input_length=max_len_char, mask_zero=True))(char_in)\n",
    "# character LSTM to get word encodings by characters\n",
    "char_enc = TimeDistributed(GRU(units=100, return_sequences=False,\n",
    "                                recurrent_dropout=0.5))(emb_char)\n",
    "\n",
    "# main LSTM\n",
    "x = concatenate([emb_word, char_enc])\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "main_lstm = Bidirectional(GRU(units=50, return_sequences=True,\n",
    "                               recurrent_dropout=0.6))(x)\n",
    "out = TimeDistributed(Dense(n_tags + 1, activation=\"sigmoid\"))(main_lstm)\n",
    "\n",
    "model = Model([word_in, char_in], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "rmsopt = RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\n",
    "model.compile(optimizer=rmsopt, loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 75, 10)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 75, 10, 100)  10500       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 75, 100)      1560800     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 75, 100)      60300       time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 75, 200)      0           embedding_1[0][0]                \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 75, 200)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 75, 100)      75300       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 75, 157)      15857       bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,722,757\n",
      "Trainable params: 1,722,757\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5083 samples, validate on 2179 samples\n",
      "Epoch 1/15\n",
      "5083/5083 [==============================] - 54s 11ms/step - loss: 0.3421 - acc: 0.9506 - val_loss: 0.0814 - val_acc: 0.9836\n",
      "Epoch 2/15\n",
      "5083/5083 [==============================] - 52s 10ms/step - loss: 0.0614 - acc: 0.9859 - val_loss: 0.0582 - val_acc: 0.9865\n",
      "Epoch 3/15\n",
      "5083/5083 [==============================] - 48s 9ms/step - loss: 0.0464 - acc: 0.9879 - val_loss: 0.0526 - val_acc: 0.9864\n",
      "Epoch 4/15\n",
      "5083/5083 [==============================] - 44s 9ms/step - loss: 0.0401 - acc: 0.9886 - val_loss: 0.0559 - val_acc: 0.9865\n",
      "Epoch 5/15\n",
      "5083/5083 [==============================] - 44s 9ms/step - loss: 0.0376 - acc: 0.9895 - val_loss: 0.0498 - val_acc: 0.9869\n",
      "Epoch 6/15\n",
      "5083/5083 [==============================] - 44s 9ms/step - loss: 0.0355 - acc: 0.9899 - val_loss: 0.0546 - val_acc: 0.9870\n",
      "Epoch 7/15\n",
      "5083/5083 [==============================] - 43s 9ms/step - loss: 0.0338 - acc: 0.9903 - val_loss: 0.0475 - val_acc: 0.9885\n",
      "Epoch 8/15\n",
      "5083/5083 [==============================] - 43s 8ms/step - loss: 0.0317 - acc: 0.9910 - val_loss: 0.0480 - val_acc: 0.9875\n",
      "Epoch 9/15\n",
      "5083/5083 [==============================] - 42s 8ms/step - loss: 0.0304 - acc: 0.9912 - val_loss: 0.0475 - val_acc: 0.9885\n",
      "Epoch 10/15\n",
      "5083/5083 [==============================] - 42s 8ms/step - loss: 0.0297 - acc: 0.9917 - val_loss: 0.0517 - val_acc: 0.9874\n",
      "Epoch 11/15\n",
      "5083/5083 [==============================] - 44s 9ms/step - loss: 0.0291 - acc: 0.9918 - val_loss: 0.0506 - val_acc: 0.9879\n",
      "Epoch 12/15\n",
      "1984/5083 [==========>...................] - ETA: 23s - loss: 0.0245 - acc: 0.9929"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_word_tr,\n",
    "                     np.array(X_char_tr).reshape((len(X_char_tr), max_len, max_len_char))],\n",
    "                    np.array(y_tr).reshape(len(y_tr), max_len, 1),\n",
    "                    batch_size=32, epochs=15, validation_data=([X_word_te,\n",
    "                     np.array(X_char_te).reshape((len(X_char_te), max_len, max_len_char))],\n",
    "                    np.array(y_te).reshape(len(y_te), max_len, 1)),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist.to_csv('../data/results/performance/SO_CHAR_GRU_Based.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(hist[\"loss\"], label='Training Loss')\n",
    "plt.plot(hist[\"val_loss\"], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(hist[\"acc\"], label='Training Accuracy')\n",
    "plt.plot(hist[\"val_acc\"], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicted = model.predict([X_word_te,\n",
    "                        np.array(X_char_te).reshape((len(X_char_te),\n",
    "                                                     max_len, max_len_char))])\n",
    "true = []\n",
    "predicted_val = []\n",
    "word = []\n",
    "for i in range(len(predicted)):\n",
    "    p_tmp = np.argmax(predicted[i], axis=-1)\n",
    "    t_tmp = y_te[i]\n",
    "    word.append(X_word_te[i])\n",
    "    predicted_val.append(p_tmp)\n",
    "    true.append(t_tmp)\n",
    "\n",
    "print word[2]\n",
    "print predicted_val[2]\n",
    "print true[2]\n",
    "\n",
    "wd = [list(map(lambda x: idx2word[x], y)) for y in word]    \n",
    "p = [list(map(lambda x: idx2tag[x], y)) for y in predicted_val]\n",
    "t = [list(map(lambda x: idx2tag[x], y)) for y in true]\n",
    "\n",
    "\n",
    "print wd[2]\n",
    "print p[2]\n",
    "print t[2]\n",
    "\n",
    "report = classification_report(list(itertools.chain.from_iterable(t)), \n",
    "                                    list(itertools.chain.from_iterable(p)))\n",
    "print \"\\n\"\n",
    "print report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 300\n",
    "pred = np.argmax(predicted[i], axis=-1)\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, tr, pred in zip(X_word_te[i], y_te[i], pred):\n",
    "    if w != 0:\n",
    "        print(\"{:15}: {:5} {}\".format(words[w], idx2tag[tr], idx2tag[pred]))\n",
    "\n",
    "# pd = model.predict(np.array([X_te[i]]))\n",
    "# pd = np.argmax(pd, axis=-1)\n",
    "# true = np.argmax(y_te[i], -1)\n",
    "# print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "# print(30 * \"=\")\n",
    "\n",
    "# for w, t1, pred in zip(X_te[i], true, pd[0]):\n",
    "#     if w != 0:\n",
    "#         print(\"{:15}: {:5} {}\".format(words[w-1], tags[t1], tags[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "{\n",
    "    'Predicted' : list(itertools.chain.from_iterable(p)),\n",
    "    'Ground Truth': list(itertools.chain.from_iterable(t)),\n",
    "    'Word' : list(itertools.chain.from_iterable(wd)),\n",
    "}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['Word'] != 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/results/SO_CHAR_GRU_Based.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run ./accuracy/src/computeSim.py ../data/results/SO_CHAR_GRU_Based.tsv ../data/validation_data/SO_AllSubsumers.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
