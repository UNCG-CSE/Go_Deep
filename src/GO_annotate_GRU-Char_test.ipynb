{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, GRU, LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import itertools\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys, os\n",
    "import traceback\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import progressbar\n",
    "\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(rootdir):\n",
    "    data_list = []\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for fname in files:\n",
    "            #print os.path.join(subdir, file)\n",
    "            filepath = subdir + os.sep + fname\n",
    "            with open(filepath) as f:\n",
    "                for line1,line2 in itertools.izip_longest(*[f]*2):\n",
    "                    try:\n",
    "                        text = ast.literal_eval(line1)\n",
    "                        label = ast.literal_eval(line2)\n",
    "                        \n",
    "#                         for i in range(len(label)):\n",
    "#                             if label[i] == 'NA':\n",
    "#                                 label[i] = 'O'\n",
    "                        n_labels = set(label)\n",
    "                        if len(text) > 3 and len(n_labels) > 1:\n",
    "                            sentence = zip(text, label)\n",
    "                            sentence.append(('.', 'O'))\n",
    "                            sentence = [item for item in sentence if item[0].strip() != '']\n",
    "                            data_list.append(sentence)\n",
    "                    except:\n",
    "                        pass\n",
    "            f.close()\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'O'), ('Hybrid', 'O'), ('Photoreceptor', 'O'), ('Expressing', 'GO:0010467'), ('Both', 'O'), ('Rod', 'O'), ('and', 'O'), ('Cone', 'O'), ('Genes', 'O'), ('in', 'O'), ('a', 'O'), ('Mouse', 'O'), ('Model', 'O'), ('of', 'O'), ('Enhanced', 'O'), ('SCone', 'O'), ('Syndrome', 'O'), ('.', 'O')]\n",
      "9571\n",
      "17921\n",
      "359\n"
     ]
    }
   ],
   "source": [
    "data_list = read_data('../data/input/GO/merged/')\n",
    "\n",
    "print data_list[0]\n",
    "\n",
    "# data_list = data_list[:3000]\n",
    "\n",
    "# words = list(chain.from_iterable(data_list))\n",
    "# print words\n",
    "\n",
    "print len(data_list)\n",
    "\n",
    "words = list(set(y[0] for x in data_list for y in x))\n",
    "words.append(\"ENDPAD\")\n",
    "n_words = len(words)\n",
    "print n_words\n",
    "\n",
    "tags = list(set(y[1] for x in data_list for y in x))\n",
    "n_tags = len(tags)\n",
    "print n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# le.fit(list(y[1] for x in data_list for y in x))\n",
    "\n",
    "# labels2idx = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "# idx2la = {labels2idx[k]:k for k in labels2idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "max_len = 75\n",
    "max_len_char = 10\n",
    "\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1\n",
    "word2idx[\"O\"] = 0\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "tag2idx = {t: i + 1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"O\"] = 0\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "\n",
    "print word2idx['Hybrid']\n",
    "print tag2idx[\"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GO:0007599': 1, 'GO:0007596': 2, 'GO:0006900': 3, 'GO:0001501': 4, 'GO:0001503': 5, 'GO:0006909': 6, 'GO:0001966': 296, 'GO:0000003': 8, 'GO:0043209': 9, 'GO:0005835': 10, 'GO:0005833': 11, 'GO:0016458': 12, 'GO:0016459': 13, 'GO:0018032': 14, 'GO:0006281': 15, 'GO:0006289': 16, 'GO:0007283': 17, 'GO:0007286': 18, 'GO:0001756': 19, 'GO:0001750': 20, 'GO:0000718': 21, 'GO:0045251': 22, 'GO:0006271': 23, 'GO:0009566': 24, 'GO:0020021': 25, 'GO:0006457': 26, 'GO:0000811': 27, 'GO:0005488': 28, 'GO:0050789': 29, 'GO:0042555': 30, 'GO:0042552': 31, 'GO:0043679': 32, 'GO:0006887': 33, 'GO:0005712': 34, 'GO:0005654': 35, 'GO:0005656': 36, 'GO:0005657': 37, 'GO:0032259': 38, 'GO:0043025': 39, 'GO:0019882': 40, 'GO:0007620': 41, 'GO:0009305': 42, 'GO:0007050': 43, 'GO:0043412': 44, 'GO:0043413': 45, 'GO:0000237': 46, 'GO:0000239': 47, 'GO:0000238': 48, 'GO:0043204': 49, 'GO:0030674': 50, 'GO:0005776': 186, 'GO:0005871': 52, 'GO:0019814': 53, 'GO:0005874': 54, 'GO:0045177': 306, 'GO:0043489': 56, 'GO:0007129': 58, 'GO:0050975': 59, 'GO:0008283': 60, 'GO:0005912': 61, 'GO:0005840': 313, 'GO:0031424': 63, 'GO:0016528': 64, 'GO:0042592': 65, 'GO:0001889': 66, 'GO:0022403': 67, 'GO:0022401': 68, 'GO:0043631': 69, 'GO:0035282': 70, 'GO:0050909': 252, 'GO:0006096': 72, 'GO:0051301': 79, 'GO:0006094': 74, 'GO:0005610': 75, 'GO:0008218': 71, 'GO:0005966': 77, 'GO:0043065': 78, 'GO:0007276': 73, 'GO:0000279': 80, 'GO:0007602': 57, 'GO:0040011': 82, 'GO:0001837': 83, 'GO:0005764': 84, 'GO:0032456': 85, 'GO:0043111': 86, 'GO:0006415': 87, 'GO:0006413': 176, 'GO:0006412': 89, 'GO:0048675': 90, 'GO:0007618': 91, 'GO:0007612': 92, 'GO:0048771': 93, 'GO:0007610': 94, 'GO:0007164': 95, 'GO:0007165': 96, 'GO:0009293': 97, 'GO:0009294': 98, 'GO:0016028': 99, 'GO:0016021': 100, 'GO:0007369': 101, 'GO:0006936': 102, 'GO:0000792': 103, 'GO:0000791': 104, 'GO:0000795': 105, 'GO:0042995': 106, 'GO:0042611': 107, 'GO:0031507': 108, 'GO:0009790': 109, 'GO:0005829': 110, 'GO:0005902': 111, 'GO:0016567': 112, 'GO:0043627': 113, 'GO:0030849': 114, 'GO:0018027': 115, 'GO:0018023': 116, 'GO:0050821': 117, 'GO:0001763': 118, 'GO:0005929': 119, 'GO:0030286': 120, 'GO:0051179': 121, 'GO:0007530': 122, 'GO:0046911': 123, 'GO:0009887': 124, 'GO:0050793': 125, 'GO:0060073': 126, 'GO:0051235': 127, 'GO:0051234': 128, 'GO:0006897': 129, 'GO:0005643': 130, 'GO:0050896': 131, 'GO:0050890': 132, 'GO:0050892': 133, 'GO:0050893': 134, 'GO:0017086': 135, 'GO:0006517': 136, 'GO:0046983': 137, 'GO:0033592': 138, 'GO:0031099': 139, 'GO:0005585': 140, 'GO:0005581': 141, 'GO:0000806': 142, 'GO:0000805': 143, 'GO:0001944': 144, 'GO:0001947': 145, 'GO:0002027': 146, 'GO:0002024': 147, 'GO:0048821': 148, 'GO:0008037': 149, 'GO:0008039': 150, 'GO:0014069': 151, 'GO:0007132': 152, 'GO:0007131': 153, 'GO:0007613': 196, 'GO:0043473': 154, 'GO:0008380': 328, 'GO:0016234': 156, 'GO:0045202': 157, 'GO:0046959': 158, 'GO:0001525': 159, 'GO:0005819': 160, 'GO:0005813': 161, 'GO:0006323': 162, 'GO:0016246': 163, 'GO:0005856': 208, 'GO:0031128': 165, 'GO:0060033': 167, 'GO:0031649': 168, 'GO:0022411': 169, 'GO:0007268': 170, 'GO:0042220': 171, 'GO:0050918': 322, 'GO:0022008': 173, 'GO:0008610': 174, 'GO:0045277': 175, 'GO:0005768': 88, 'GO:0000240': 177, 'GO:0019226': 178, 'GO:0005783': 179, 'GO:0006473': 180, 'GO:0019228': 181, 'GO:0030056': 182, 'GO:0030054': 183, 'GO:0005773': 184, 'GO:0005777': 185, 'GO:0001822': 51, 'GO:0048511': 187, 'GO:0048513': 188, 'GO:0005675': 189, 'GO:0048518': 190, 'GO:0048519': 191, 'GO:0007608': 192, 'GO:0007605': 193, 'GO:0007601': 194, 'GO:0007600': 195, 'GO:0006605': 81, 'GO:0045184': 197, 'GO:0042756': 198, 'GO:0010369': 199, 'GO:0001917': 200, 'GO:0002076': 201, 'GO:0022607': 202, 'GO:0006928': 203, 'GO:0000785': 204, 'GO:0000786': 205, 'GO:0021700': 206, 'GO:0043226': 207, 'GO:0019835': 164, 'GO:0003077': 209, 'GO:0016477': 210, 'GO:0006810': 211, 'GO:0050953': 212, 'GO:0008305': 166, 'GO:0012501': 214, 'GO:0001775': 215, 'GO:0050817': 216, 'GO:0045333': 217, 'GO:0006954': 218, 'GO:0050819': 219, 'GO:0006952': 213, 'GO:0046903': 221, 'GO:0008091': 222, 'GO:0051604': 223, 'GO:0043093': 224, 'GO:0000732': 225, 'GO:0006350': 226, 'GO:0006353': 227, 'GO:0016020': 228, 'GO:0030154': 229, 'GO:0031012': 230, 'GO:0030016': 231, 'GO:0060047': 232, 'GO:0060041': 233, 'GO:0030010': 234, 'GO:0007059': 235, 'GO:0051325': 236, 'GO:0051324': 237, 'GO:0051320': 238, 'GO:0051323': 239, 'GO:0005730': 240, 'GO:0001708': 241, 'GO:0001709': 242, 'GO:0005739': 243, 'GO:0006508': 244, 'GO:0032502': 245, 'GO:0001570': 220, 'GO:0032508': 246, 'GO:0016485': 247, 'GO:0051606': 248, 'GO:0051258': 249, 'GO:0050777': 250, 'GO:0050779': 251, 'independent_continuant': 253, 'GO:0043500': 254, 'GO:0007389': 255, 'GO:0035212': 256, 'GO:0007586': 257, 'GO:0006914': 258, 'GO:0006915': 259, 'GO:0006911': 260, 'GO:0009058': 261, 'GO:0046581': 262, 'GO:0009056': 263, 'GO:0000775': 264, 'GO:0006310': 265, 'GO:0006311': 266, 'GO:0000075': 267, 'GO:0045123': 268, 'GO:0045120': 269, 'GO:0030908': 270, 'GO:0060004': 271, 'GO:0035102': 272, 'GO:0035106': 273, 'GO:0030425': 274, 'GO:0030424': 275, 'GO:0051899': 276, 'GO:0007517': 277, 'GO:0031982': 278, 'GO:0006266': 279, 'GO:0006260': 280, 'GO:0032391': 281, 'GO:0005792': 282, 'GO:0031965': 283, 'GO:0030164': 284, 'GO:0000502': 285, 'GO:0051216': 286, 'GO:0018991': 287, 'GO:0048532': 347, 'GO:0051866': 289, 'GO:0043131': 290, 'GO:0030194': 291, 'GO:0007631': 292, 'GO:0031240': 293, 'GO:0007067': 294, 'GO:0035315': 295, 'GO:0065003': 7, 'GO:0001967': 297, 'GO:0065007': 298, 'GO:0019233': 299, 'GO:0048471': 300, 'GO:0019230': 301, 'GO:0048477': 302, 'GO:0016265': 303, 'GO:0005737': 304, 'GO:0042384': 305, 'GO:0007126': 55, 'O': 0, 'GO:0008152': 308, 'GO:0043256': 309, 'GO:0042730': 310, 'GO:0045165': 311, 'GO:0031648': 312, 'GO:0007114': 62, 'GO:0016540': 314, 'GO:0005634': 315, 'GO:0016310': 316, 'GO:0031214': 351, 'GO:0030261': 318, 'GO:0005901': 319, 'GO:0030263': 320, 'GO:0001669': 321, 'GO:0043626': 172, 'GO:0007411': 323, 'GO:0009653': 324, 'GO:0007416': 325, 'GO:0006349': 326, 'GO:0022610': 327, 'GO:0051304': 76, 'GO:0022616': 155, 'GO:0031143': 329, 'GO:0051318': 330, 'GO:0051312': 331, 'GO:0045453': 332, 'GO:0045298': 333, 'GO:0007568': 334, 'GO:0007569': 335, 'GO:0007565': 336, 'GO:0007566': 337, 'GO:0007567': 338, 'GO:0005623': 339, 'GO:0005622': 340, 'GO:0050729': 341, 'GO:0030097': 342, 'GO:0000267': 343, 'GO:0040007': 344, 'GO:0050879': 345, 'GO:0010467': 346, 'GO:0005694': 288, 'GO:0009986': 348, 'GO:0048666': 349, 'GO:0005883': 350, 'GO:0005886': 317, 'GO:0007155': 352, 'GO:0007154': 353, 'GO:0042470': 354, 'GO:0032991': 355, 'GO:0007399': 356, 'GO:0005575': 357, 'GO:0005577': 358, 'GO:0005576': 359}\n"
     ]
    }
   ],
   "source": [
    "print tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_word = [[word2idx[w[0]] for w in s] for s in data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_word = pad_sequences(maxlen=max_len, sequences=X_word, value=word2idx[\"O\"], padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "chars = set([w_i for w in words for w_i in w])\n",
    "n_chars = len(chars)\n",
    "print(n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char2idx = {c: i + 2 for i, c in enumerate(chars)}\n",
    "char2idx[\"UNK\"] = 1\n",
    "char2idx[\"O\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_char = []\n",
    "for sentence in data_list:\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                word_seq.append(char2idx.get(sentence[i][0][j]))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"PAD\"))\n",
    "        sent_seq.append(word_seq)\n",
    "    X_char.append(np.array(sent_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = [[tag2idx[w[1]] for w in s] for s in data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = pad_sequences(maxlen=max_len, sequences=y,value=tag2idx[\"O\"], padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y = [to_categorical(i, num_classes=n_tags+1) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_word_tr, X_word_te, y_tr, y_te = train_test_split(X_word, y, test_size=0.3, random_state=2018)\n",
    "X_char_tr, X_char_te, _, _ = train_test_split(X_char, y, test_size=0.3, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D\n",
    "from keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "# input and embedding for words\n",
    "word_in = Input(shape=(max_len,))\n",
    "emb_word = Embedding(input_dim=n_words + 2, output_dim=100,\n",
    "                     input_length=max_len, mask_zero=True)(word_in)\n",
    "\n",
    "# input and embeddings for characters\n",
    "char_in = Input(shape=(max_len, max_len_char,))\n",
    "emb_char = TimeDistributed(Embedding(input_dim=n_chars + 2, output_dim=30,\n",
    "                           input_length=max_len_char, mask_zero=True))(char_in)\n",
    "# character LSTM to get word encodings by characters\n",
    "char_enc = TimeDistributed(GRU(units=150, return_sequences=False,\n",
    "                                recurrent_dropout=0.5))(emb_char)\n",
    "\n",
    "# main LSTM\n",
    "x = concatenate([emb_word, char_enc])\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "main_lstm = Bidirectional(GRU(units=150, return_sequences=True,\n",
    "                               recurrent_dropout=0.6))(x)\n",
    "model = TimeDistributed(Dense(n_tags + 1, activation=\"sigmoid\"))(main_lstm)\n",
    "\n",
    "crf = CRF(n_tags+1)  # CRF layer\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model([word_in, char_in], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 75, 10)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 75, 10, 30)   3180        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 75, 100)      1792300     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 75, 150)      81450       time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 75, 250)      0           embedding_1[0][0]                \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 75, 250)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 75, 300)      360900      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 75, 360)      108360      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, 75, 360)      260280      time_distributed_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 2,606,470\n",
      "Trainable params: 2,606,470\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6699 samples, validate on 2872 samples\n",
      "Epoch 1/15\n",
      "2272/6699 [=========>....................] - ETA: 55s - loss: 17.0819 - viterbi_acc: 0.8965"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_word_tr,\n",
    "                     np.array(X_char_tr).reshape((len(X_char_tr), max_len, max_len_char))],\n",
    "                    np.array(y_tr),\n",
    "                    batch_size=32, epochs=15, validation_data=([X_word_te,\n",
    "                     np.array(X_char_te).reshape((len(X_char_te), max_len, max_len_char))],\n",
    "                    np.array(y_te)),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist.to_csv('../data/results/performance/GO_merged_CHAR_GRU_Based_test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(hist[\"loss\"], label='Training Loss')\n",
    "plt.plot(hist[\"val_loss\"], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(hist[\"viterbi_acc\"], label='Training Accuracy')\n",
    "plt.plot(hist[\"val_viterbi_acc\"], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicted = model.predict([X_word_te,\n",
    "                        np.array(X_char_te).reshape((len(X_char_te),\n",
    "                                                     max_len, max_len_char))])\n",
    "true = []\n",
    "predicted_val = []\n",
    "word = []\n",
    "for i in range(len(predicted)):\n",
    "    p_tmp = np.argmax(predicted[i], axis=-1)\n",
    "    t_tmp = np.argmax(y_te[i], -1)\n",
    "    word.append(X_word_te[i])\n",
    "    predicted_val.append(p_tmp)\n",
    "    true.append(t_tmp)\n",
    "\n",
    "print word[2]\n",
    "print predicted_val[2]\n",
    "print true[2]\n",
    "\n",
    "wd = [list(map(lambda x: idx2word[x], y)) for y in word]    \n",
    "p = [list(map(lambda x: idx2tag[x], y)) for y in predicted_val]\n",
    "t = [list(map(lambda x: idx2tag[x], y)) for y in true]\n",
    "\n",
    "\n",
    "print wd[2]\n",
    "print p[2]\n",
    "print t[2]\n",
    "\n",
    "report = classification_report(list(itertools.chain.from_iterable(t)), \n",
    "                                    list(itertools.chain.from_iterable(p)))\n",
    "print \"\\n\"\n",
    "print report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 300\n",
    "pred = np.argmax(predicted[i], axis=-1)\n",
    "print pred\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, tr, pred in zip(X_word_te[i], np.argmax(y_te[i], -1), pred):\n",
    "    if w != 0:\n",
    "        print(\"{:15}: {:5} {}\".format(words[w], idx2tag[tr], idx2tag[pred]))\n",
    "        \n",
    "# pd = model.predict(np.array([X_te[i]]))\n",
    "# pd = np.argmax(pd, axis=-1)\n",
    "# true = np.argmax(y_te[i], -1)\n",
    "# print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "# print(30 * \"=\")\n",
    "\n",
    "# for w, t1, pred in zip(X_te[i], true, pd[0]):\n",
    "#     if w != 0:\n",
    "#         print(\"{:15}: {:5} {}\".format(words[w-1], tags[t1], tags[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "{\n",
    "    'Predicted' : list(itertools.chain.from_iterable(p)),\n",
    "    'Ground Truth': list(itertools.chain.from_iterable(t)),\n",
    "    'Word' : list(itertools.chain.from_iterable(wd)),\n",
    "}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['Word'] != 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/results/GO_merged_CHAR_GRU_Based_test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./accuracy/src/computeSim.py ../data/results/GO_merged_CHAR_GRU_Based_test.tsv ../data/validation_data/GO_AllSubsumers.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
